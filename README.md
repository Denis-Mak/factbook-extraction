Factbook internet crawler and gathering new facts
=================================================

Основная функция этого проекта - заполнять базу и индексы фактами на темы, которые заинтересовали пользователей.
Процесс заполения построен как цепочка (или конвейер обработчиков), связанных общей шиной сообщений. Каждый обработчик
выполняет свою атомарную функцию и кидает сообщение даль по шине (или нет, если это заключительный обработчик в цепочке)
В двух словах процесс выглядит так:
1. на вход поступает профиль
2. из всех словосочетаний профиля формируются запросы в поисковые системы (Google, Yahoo, Faroo)
3. поисковые результаты обрабатываются и все ссылки, которые еще не были загруженны в систему, скачиваются
4. закаченные страницы обрабатываются: определяется тип страницы (HTML/PDF), выделяется главная статья, определяется дата публикации
5. текст с основными атрибутами разбирается на факты (Fact)
6. факты сохраняются в базу
7. новые факты попадают в контекстный и семантический индексы

В качестве дополнительной функции на этот же проект легла обработка обновления проекта по добавленному факту, которая
выполнена в виде RPC (Remote Procedure Call). Фактически само обновление происходит при помощи библиотеки factbook-search/
но функция размещена здесь, а не в Web-приложении для того чтобы не держать в двух местах словари в памяти, которые занимают
ни много ни мало 5 Гбайт.


### Список обработчиков и их функции
#### Клиент для поисковых движков
Получает на вход профиль и запускает поисковые запросы к поисковым движкам, которые подключены в конфигурации.
В данный момент подключены клиенты:
* [Google](https://developers.google.com/custom-search/docs/api)
* [Yahoo](https://developer.yahoo.com/boss/search/)
* [Bing](http://msdn.microsoft.com/en-us/library/dd251056.aspx)
* [Faroo](http://www.faroo.com/hp/api/api.html)

Все клиенты размещены в пакете <code>it.factbook.extraction.client</code>.
Настройки подключения к поисковым движкам лежат в файле <code>resourses/ampq.properties</code>

Все клиенты написаны на базе класса AbstractSearchEngineClient и в простейшем случае чтобы добавить нового клиента
нужно сделать для его запись в enum SearchEngine, отнаследовать класс от AbstractSearchEngineClient и написать реализацию
метода getLinks. В сложном - перекрыть остальные методы AbstractSearchEngineClient. Ну и конечно не забыть подключить
нового клиента как обработчик к топику RabbitMQ.

Перед добавлением ссылки в список для скачивания проверяет не была ли эта ссылка закачена ранее.

#### Crawler
Скачивает страницу из Интернет по заданному URL, определяет тип статьи (HTML/PDF), разбирает ее, выделяет главную
статью, выделяет дату публикации, пересылает дальше текст статьи и ее мета-данные (тип, дату публикации, размер).

Обработчик построен на библиотеке [Apache Tika](http://tika.apache.org/) собственной сборки, в которой была заменена
только версия BoilerPipe, вероятнее всего современные версии по-умолчанию включают правильную версию.
Выделение текста главной статьи сделано при помощи библиотеки [BoilerPipe](https://github.com/kohlschutter/boilerpipe)
Выделение даты публикации - библиотека [DCTExtractor](https://github.com/xtannier/DCTFinder), которой в свою очередь
необходима для работы программа [wapity](https://github.com/Jekub/Wapiti)

При обработке используются следующие настройки:
 - ожидание ответа от страницы - 1.5 сек
 - ожидание загрузки страницы - 30 сек

 если время превышает эти значения, то соединение принудительно разрывается.

 - статья обрабатывается если в ней до 100000 знаков, в противном случае она отклоняется с записью warning-а в лог.

 Все закачки журналируются в базе, чтобы не скачивать страницы повторно для работы Crawler оперирует таблицей CrawlerLog из
  схемы extraction в базе MySQL

#### FactSaver
Разбирает текст статьи на факты и сохраняет в базу. Обработчик только пользуется сторонними классами из библиотеки
factbook-search для разбора, в нем самом только логика предварительной очистки текста статьи.

#### IndexUpdater
Добавляет факты в индексы контекстного и семантического поиска.

#### FactClassifierTrainer
Обучает классификатор фактов, который отделяет содержательные факты от спам-а.
**На данный момент выключен.**


Есть еще два обработчика, которые не входят в конвейер обработки закаченных статей, они вызываются как удаленные методы (RPC).
#### ProfileUpdaterMessageHandler
Принимает на входе сообщение с профилем и фактом, который в него сохранили. Разбирает факт и добавляет знаковые словосочетания
в профиль. Использует теже классы из factbook-search для разбора.

#### TreeBuilder
Разбирает факт, строит его синтаксические деревья и выдает их в формате удобном для печати.

### Шина сообщений
Шина сообщений построена на [RabbitMQ](https://www.rabbitmq.com/) и библиотеке [Spring-AMQP](http://projects.spring.io/spring-amqp/)
В шине сообщений есть несколько топиков, в которые пишутся сообщения, и очереди, связанные с топиками, на которые
подписаны обработчики, которые считывают сообщения. Как только все обработчики получили сообщение, оно удаляется из очереди.
Список топиков и очередей можно увидеть в классе <code>config/AmqpCongig</code>

### Запуск и мониторинг приложения
Для запуска приложения используется shell-скрипт для Windows: <code>install-service.bat</code> и <code>uninstall-service.bat</code>
; для Linux: factbook-extraction-start.sh

Во всех ОС приложение работает как фоновый сервис, в случае Windows управляется из Administration tools - Services,
в Linux запуск: <code>factbook-extraction-start.sh start</code> остановка: <code>factbook-extraction-start.sh stop</code>

### Javadoc reference
 * [Javadoc API - current version](http://denis-mak.github.io/factbook-extraction/current/docs/index.html)

### Структура данных
* [Модель данных MySQL (схема extraction))](http://denis-mak.github.io/factbook/current/datamodel/factbook-data-structure.png)